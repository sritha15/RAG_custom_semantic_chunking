{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# üîç Retrieval-Augmented Generation (RAG) with Custom Semantic Chunking\n",
        "\n",
        "This notebook implements a robust RAG pipeline using a document-heavy scientific report. The system includes:\n",
        "- Advanced PDF parsing and cleaning\n",
        "- Semantic and Custom semantic anchor-based chunking strategy\n",
        "- Embedding and indexing using ChromaDB\n",
        "- Retrieval with OpenAI GPT-4 for grounded Q&A\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## ‚ú® Explainability & Methodology Overview\n"
      ],
      "metadata": {
        "id": "brrqoXVrtMPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### üìÑ Text Cleaning & Preprocessing\n",
        "\n",
        "The uploaded EEAP report PDF was parsed using `pdfplumber`. Key steps:\n",
        "- Removed hyperlinks (to avoid non-informative tokens)\n",
        "- Normalized whitespace and joined hyphenated line breaks\n",
        "- Skipped table of contents pages\n",
        "- Optionally removed unwanted sections like \"Executive Summary\"\n",
        "\n",
        "This results in a clean, structured body of text for downstream processing."
      ],
      "metadata": {
        "id": "TK8jJvQgtRNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### üß† Custom Semantic Chunking Strategy\n",
        "\n",
        "Unlike traditional sequential or fixed-size chunking, this approach:\n",
        "- Uses an `anchor_stride` to select anchor sentences\n",
        "- Computes cosine similarity between each anchor and all sentences (pre-combined in `comb`)\n",
        "- Selects the most semantically similar, non-overlapping sentences until a chunk size limit is reached\n",
        "\n",
        "This allows the model to retrieve **cross-paragraph** and **contextually linked** ideas ‚Äî critical for scientific or long-form content.\n"
      ],
      "metadata": {
        "id": "XFMpVJPltVyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### üß™ Embedding + Retrieval + LLM (RAG)\n",
        "\n",
        "- `SentenceTransformer` is used for embedding chunks\n",
        "- ChromaDB is used for vector indexing and fast approximate retrieval\n",
        "- GPT-4 (via OpenAI API) is used to answer questions grounded in the top-k retrieved chunks\n",
        "\n",
        "This architecture enables accurate, explainable, and flexible QA over large documents.\n",
        "\n",
        "You can now evaluate the performance differences between chunking strategies using a set of benchmark questions or LLM scoring.\n"
      ],
      "metadata": {
        "id": "rK42OGAjtX5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install PyPDF2\n",
        "!pip install chromadb --upgrade\n",
        "!pip install --upgrade openai\n",
        "import re\n",
        "import openai\n",
        "import chromadb\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "l_wUYQ2Rtaf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PDF Preprocessing**"
      ],
      "metadata": {
        "id": "9Vxz2h50i0ex"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Vl3G1j6tTI-"
      },
      "outputs": [],
      "source": [
        "pdf_path = \"EEAP-2022-Assessment-Report-May2023-1-30.pdf\"\n",
        "cleaned_pages = []\n",
        "\n",
        "\n",
        "skip_pages = set(range(7, 11)).union(range(2, 3))\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    for i, page in enumerate(pdf.pages):\n",
        "        if i in skip_pages:\n",
        "            continue\n",
        "\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            text = re.sub(r'https?://\\S+', '', text)\n",
        "            text = re.sub(r'\\s+', ' ', text)\n",
        "            text = re.sub(r'-\\s+', '', text)\n",
        "            text = re.sub(r'\\s{2,}', ' ', text)\n",
        "            cleaned_pages.append(text.strip())\n",
        "\n",
        "full_clean_text = \"\\n\\n\".join(cleaned_pages)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_clean_text)"
      ],
      "metadata": {
        "id": "jw43nkKxvMp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_clean_text=full_clean_text.lower().replace('executive summary','')"
      ],
      "metadata": {
        "id": "WzcoZmq2yggb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_clean_text)"
      ],
      "metadata": {
        "id": "_KCbO6_AyK7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s=re.split(r'(?<=[.!?])\\s+|\\n{2,}', full_clean_text.strip())"
      ],
      "metadata": {
        "id": "vaYF5z__BnkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(s)"
      ],
      "metadata": {
        "id": "TW5wC1vSgrz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[0]"
      ],
      "metadata": {
        "id": "HJX9BYNshzXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=[]\n",
        "for i, j in enumerate(s):\n",
        "  sentences.append({'sentences':j, 'index':i})"
      ],
      "metadata": {
        "id": "OThTDsK_emvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "id": "niYlZTkaeqXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "func is a function desined for getting anchor text for Semantic Chunking prep. We take a sentence as an anchor and get the buffer_size number of sentences around it and put it into a list called comb, which will later be used."
      ],
      "metadata": {
        "id": "p8li9-1mjV5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func(sentences, buffer_size):\n",
        "  combined=[]\n",
        "  for index,value in enumerate(sentences):\n",
        "    val=''\n",
        "    for o1 in range(index-min(index,buffer_size),index+1):\n",
        "      val=val+sentences[o1]['sentences']\n",
        "    k=val\n",
        "    val=''\n",
        "    for o2 in range(index+1,min(len(sentences),buffer_size+1+index)):\n",
        "      val=val+sentences[o2]['sentences']\n",
        "    k=k+val\n",
        "\n",
        "    combined.append(k)\n",
        "  return combined\n"
      ],
      "metadata": {
        "id": "0gnpSXecDUlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comb=func(sentences,buffer_size= 1)"
      ],
      "metadata": {
        "id": "9Y1hveaEeadJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[0]"
      ],
      "metadata": {
        "id": "x-e-m4Y5fCMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[1]"
      ],
      "metadata": {
        "id": "9ayYgKQsfVLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comb[0]"
      ],
      "metadata": {
        "id": "mAS_sYAvefLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comb[1]"
      ],
      "metadata": {
        "id": "VoQEEsZ_g7tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comb"
      ],
      "metadata": {
        "id": "EgsZGrIUfSKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Chunking Strategy Descriptions\n",
        "\n",
        "### üß± Sequential + Buffer Chunking\n",
        "This strategy breaks the document into chunks by moving linearly through the text, combining each sentence with a fixed number of neighboring sentences (the buffer) on either side. It preserves local coherence and is simple to implement, making it effective when important context is typically found nearby. However, it may miss deeper semantic relationships between sentences that aren't adjacent, especially in documents with dispersed or cross-referenced content.\n",
        "\n",
        "### üß† Anchor-Based Semantic Chunking\n",
        "In this approach, selected anchor sentences serve as the center of each chunk. For each anchor, the system computes semantic similarity with all surrounding sentence groups (pre-computed via the buffer logic) and gathers the most relevant ones, regardless of their original order in the document. This allows the chunk to contain high-context, meaningfully related content even from non-contiguous sections. It results in richer and more focused retrieval, particularly useful in complex documents with interrelated topics.\n"
      ],
      "metadata": {
        "id": "wKXbVaKW1EHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now using the all-MiniLM-L6-v2 model to get eh embeddings of the sentences in the comb variable, and we use the sentence similarity to figure out which sentences to put in the same chunk based on a threshold and chunk limit. chunk limit by default is 512."
      ],
      "metadata": {
        "id": "I2Hxul6jk1B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def semantic_chunking(comb, threshold=0.75, chunk_limit=None):\n",
        "    if chunk_limit is None:\n",
        "        chunk_limit = 512\n",
        "\n",
        "    embeddings = model.encode(comb)\n",
        "    chunks = []\n",
        "    current_chunk = [comb[0]]\n",
        "    current_len = len(comb[0].split())\n",
        "\n",
        "    for i in range(1, len(comb)):\n",
        "        sim = cosine_similarity([embeddings[i]], [embeddings[i - 1]])[0][0]\n",
        "        next_len = len(comb[i].split())\n",
        "\n",
        "        if sim > threshold and current_len + next_len <= chunk_limit:\n",
        "            current_chunk.append(comb[i])\n",
        "            current_len += next_len\n",
        "        else:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [comb[i]]\n",
        "            current_len = next_len\n",
        "\n",
        "    chunks.append(\" \".join(current_chunk))\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "fAHTlUPJgStM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_chunks=semantic_chunking(comb)"
      ],
      "metadata": {
        "id": "ejyhTdDRJtP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(semantic_chunks)"
      ],
      "metadata": {
        "id": "aGqB25tlGQ9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(semantic_chunks)"
      ],
      "metadata": {
        "id": "5ea5RNWHK7PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=[{i: len(i.split())} for i in semantic_chunks]"
      ],
      "metadata": {
        "id": "y7V9UA0VHb8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k[0]"
      ],
      "metadata": {
        "id": "BL77-UCJGx9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k[1]"
      ],
      "metadata": {
        "id": "QMn8uJ67G1n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k[3]"
      ],
      "metadata": {
        "id": "lXxyHLabG5qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k[4]"
      ],
      "metadata": {
        "id": "SLJMK3uyG8jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k[6]"
      ],
      "metadata": {
        "id": "h7qdMOUTG-_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(max(k, key=lambda x: list(x.values())[0]).items())[0][1]"
      ],
      "metadata": {
        "id": "Jssmf4hCHD8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(api_key= \"YOUR API KEY\" )"
      ],
      "metadata": {
        "id": "-1UgRVSqTAQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "collection_seq = chroma_client.get_or_create_collection(\"seq_chunking\")\n",
        "collection_sem = chroma_client.get_or_create_collection(\"semantic_chunking\")\n"
      ],
      "metadata": {
        "id": "SdxqSfPErp7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chroma_activate(chunks, collection):\n",
        "  for i, text in enumerate(chunks):\n",
        "    embedding = model.encode([text])[0].tolist()\n",
        "    collection.add(\n",
        "        documents=[text],\n",
        "        embeddings=[embedding],\n",
        "        ids=[f\"id-{i}\"]\n",
        "    )\n",
        "  return collection\n"
      ],
      "metadata": {
        "id": "JNMHWbj1lby6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection1= chroma_activate(semantic_chunks, collection_seq)"
      ],
      "metadata": {
        "id": "YDx6Pgyllrja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = collection1.get(include=['documents', 'embeddings'])\n",
        "results['ids'][0], results['documents'][0], results['embeddings'][0]\n",
        "# for doc, emb in zip(results['documents'], results['embeddings']):\n",
        "#     print(f\"Document:\\n{doc}\\n\\nEmbedding (first 5 dims):\\n{emb[:5]}\\n{'-'*40}\")\n"
      ],
      "metadata": {
        "id": "EkOwHiHtR4lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = collection1.get(include=[\"documents\", \"embeddings\"])\n",
        "print(results[\"ids\"][:1])\n",
        "print(results[\"documents\"][:1])\n",
        "print(len(results[\"embeddings\"]))\n"
      ],
      "metadata": {
        "id": "viW9VX9uRLJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_cluster_chunking(comb, anchor_stride=5, chunk_limit=512):\n",
        "    embeddings = model.encode(comb)\n",
        "    chunks = []\n",
        "    used = set()\n",
        "\n",
        "    for i in range(0, len(comb), anchor_stride):\n",
        "        anchor_emb = embeddings[i]\n",
        "\n",
        "        similarities = cosine_similarity([anchor_emb], embeddings)[0]\n",
        "        ranked = sorted(enumerate(similarities), key=lambda x: -x[1])\n",
        "\n",
        "        chunk = []\n",
        "        word_count = 0\n",
        "\n",
        "        for idx, sim in ranked:\n",
        "            if idx in used:\n",
        "                continue\n",
        "            wc = len(comb[idx].split())\n",
        "            if word_count + wc <= chunk_limit:\n",
        "                chunk.append(comb[idx])\n",
        "                used.add(idx)\n",
        "                word_count += wc\n",
        "            if word_count >= chunk_limit:\n",
        "                break\n",
        "\n",
        "        if chunk:\n",
        "            chunks.append(\" \".join(chunk))\n",
        "\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "j94Xk2-qS58G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_with_llm(query, collection, top_k=3):\n",
        "    query_emb = model.encode(query).tolist()\n",
        "    results = collection.query(query_embeddings=[query_emb], n_results=top_k)\n",
        "    context = \"\\n\".join(results['documents'][0])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "            You are an expert assistant. Use the following context to answer the question concisely and accurately.\n",
        "            If the answer is not in the context, say 'Not enough information in the document.'\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Question: {query}\n",
        "            Answer:\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.5\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n"
      ],
      "metadata": {
        "id": "-AwRbeq-OQrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_with_llm(\"How does ozone depletion affect UV exposure?\", collection=collection1)\n"
      ],
      "metadata": {
        "id": "rDcKhlT9HPts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_with_llm(\"What is MCP?\", collection=collection1)\n"
      ],
      "metadata": {
        "id": "X76wmqERUoTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_semantic_chunks=semantic_cluster_chunking(comb)"
      ],
      "metadata": {
        "id": "wBOt6auJUxid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_semantic_chunks[0]"
      ],
      "metadata": {
        "id": "qdR3zdQoU04n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comb[0]"
      ],
      "metadata": {
        "id": "Kh_SNJlhW9YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection2= chroma_activate(new_semantic_chunks, collection_sem)"
      ],
      "metadata": {
        "id": "zXqohsskmKId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_with_llm(\"How does ozone depletion affect UV exposure?\", collection = collection2)"
      ],
      "metadata": {
        "id": "BuVgsF--eJRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_with_llm(\"What are the main health benefits attributed to the Montreal Protocol, and how were they quantified?\", collection = collection1)"
      ],
      "metadata": {
        "id": "NvKP8PFRfWB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_with_llm(\"What are the main health benefits attributed to the Montreal Protocol, and how were they quantified?\", collection = collection2)"
      ],
      "metadata": {
        "id": "6VlttX7BqTAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_with_llm(\"How does UV-B radiation interact with climate factors to affect terrestrial or aquatic ecosystems?\", collection = collection1)"
      ],
      "metadata": {
        "id": "yPjSHm4CqbSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_with_llm(\"How does UV-B radiation interact with climate factors to affect terrestrial or aquatic ecosystems?\", collection = collection2)"
      ],
      "metadata": {
        "id": "pWNpjdCuqUhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Comparison of Chunking Strategies\n",
        "\n",
        "### 1. Sequential + Buffer Chunking\n",
        "- ‚úÖ Easy to implement\n",
        "- ‚úÖ Maintains local sentence continuity\n",
        "- ‚ùå Can miss cross-paragraph context\n",
        "- ‚ùå Chunks may contain filler or loosely related content\n",
        "\n",
        "### 2. Anchor-Based Semantic Chunking (Proposed)\n",
        "- ‚úÖ Selects the most relevant context per query\n",
        "- ‚úÖ Pulls in semantically similar content even across sections\n",
        "- ‚úÖ Produces tighter, high-signal chunks\n",
        "- ‚ùå Slightly more computational overhead (semantic similarity matrix) only by a few seconds. It's not that big of a trade off once we get the chunks and create the collection\n",
        "\n",
        "### üèÜ Why the Second One Wins\n",
        "- Better relevance in retrieval: LLM responses based on these chunks are more accurate and grounded\n",
        "- Captures dispersed but related concepts (important in scientific or legal docs)\n",
        "\n",
        "### üî¨ Empirical Observation\n",
        "When tested across questions like \"What are the combined effects of UV radiation and climate?\" or \"How does the Montreal Protocol influence health outcomes?\", the anchor-semantic strategy provided richer and more concise grounding for GPT-4, resulting in clearer and more factually correct answers.\n",
        "\n"
      ],
      "metadata": {
        "id": "tiYv4BJ1xWdm"
      }
    }
  ]
}